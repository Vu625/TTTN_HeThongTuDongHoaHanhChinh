#*
# File nÃ y chá»©a lÃµi logic giao tiáº¿p trá»±c tiáº¿p vá»›i model AI (PhoGPT) thÃ´ng qua llama-cpp-python.
# NÃ³ chá»‹u trÃ¡ch nhiá»‡m cho viá»‡c táº£i model, cáº¥u hÃ¬nh tham sá»‘, vÃ  cháº¡y suy luáº­n (inference).*#

import os
import requests  # ğŸ†• ThÃªm thÆ° viá»‡n Ä‘á»ƒ gá»i API LM Studio
# from rag_numpy_phogpt import RAGEngine  # âŒ COMMENT: DÃ²ng nÃ y dÃ¹ng llama-cpp, táº¡m thá»i vÃ´ hiá»‡u khi chuyá»ƒn sang LM Studio

# ===============================
# âš™ï¸ Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN
# ===============================
#MODEL_PATH = r"PhoGPT-4B-Chat-Q4_K_M.gguf"  # âŒ DÃ²ng nÃ y chá»‰ dÃ¹ng khi cháº¡y model qua llama-cpp
#DOC_FILE = r"D:\abode\pythonProject\ChatBot_Demo\data\luat_ban_hanh_vbqppl.txt"
INDEX_FOLDER = r"D:\abode\pythonProject\ChatBot_Demo\index"

# ===============================
# ğŸš€ KHá»I Táº O RAG + PHOGPT
# ===============================
# âŒ PHIÃŠN Báº¢N CÅ¨ â€” cháº¡y model ná»™i bá»™ báº±ng llama-cpp
# rag_engine = RAGEngine(model_path=MODEL_PATH, n_threads=6)

# âœ… PHIÃŠN Báº¢N Má»šI â€” dÃ¹ng LM Studio API Ä‘á»ƒ thay tháº¿ llama-cpp
LMSTUDIO_URL = "http://127.0.0.1:1234/v1/chat/completions"  # ğŸ†• API cá»§a LM Studio
HEADERS = {"Content-Type": "application/json"}  # ğŸ†• Header cho request
# MODEL_NAME = "lmstudio-community/Meta-Llama-3-8B-Instruct"  # âš ï¸ Sá»­a tÃªn theo model Ä‘ang cháº¡y trong LM Studio
MODEL_NAME = "phogpt-4b-chat"  # âš ï¸ Sá»­a tÃªn theo model Ä‘ang cháº¡y trong LM Studio

# ===============================
# ğŸ“š KIá»‚M TRA HOáº¶C XÃ‚Y Dá»°NG CHá»ˆ Má»¤C
# ===============================
# â— RAGEngine dÃ¹ng llama-cpp nÃªn pháº§n nÃ y chá»‰ giá»¯ logic kiá»ƒm tra file
if not os.path.exists(INDEX_FOLDER) or len(os.listdir(INDEX_FOLDER)) == 0:
    print("âš™ï¸ KhÃ´ng tÃ¬m tháº¥y chá»‰ má»¥c. (TÃ­nh nÄƒng RAG táº¡m thá»i bá»‹ vÃ´ hiá»‡u khi dÃ¹ng LM Studio)")
    # âŒ CÅ©: rag_engine.build_index_from_file(DOC_FILE, save_dir=INDEX_FOLDER)
else:
    print("âœ… ÄÃ£ tÃ¬m tháº¥y chá»‰ má»¥c. (ChÆ°a tÃ­ch há»£p RAGEngine vá»›i LM Studio)")
    # âŒ CÅ©: rag_engine.load_index(INDEX_FOLDER)

# ===============================
# ğŸ’¬ HÃ€M Há»I CHATBOT â€” PHIÃŠN Báº¢N Má»šI
# ===============================
def ask_lmstudio(prompt):
    print(f"\nğŸ§  CÃ¢u há»i: {prompt}\n")
    try:
        # ğŸ†• Gá»­i prompt tá»›i LM Studio API (thay vÃ¬ cháº¡y model ná»™i bá»™)
        data = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "Báº¡n lÃ  chatbot há»— trá»£ cÃ´ng dÃ¢n Viá»‡t Nam, tráº£ lá»i ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c."},
                {"role": "user", "content": prompt}
            ]
        }

        response = requests.post(LMSTUDIO_URL, headers=HEADERS, json=data)
        answer = response.json()["choices"][0]["message"]["content"]
        print(f"\nâœ… Káº¿t luáº­n: {answer}")
        return answer
    except Exception as e:
        print(f"âŒ Lá»—i khi gá»i LM Studio API: {e}")
        return None

# ===============================
# ğŸ§‘â€ğŸ’» VÃ’NG Láº¶P CHAT
# ===============================
if __name__ == "__main__":
    print("\nğŸ’¬ ChatBot LM Studio sáºµn sÃ ng! HÃ£y Ä‘áº·t cÃ¢u há»i (gÃµ 'exit' hoáº·c 'thoÃ¡t' Ä‘á»ƒ dá»«ng).")
    while True:
        question = input("\nBáº¡n: ").strip()
        if question.lower() in ["exit", "quit", "q", "thoÃ¡t"]:
            print("ğŸ‘‹ Táº¡m biá»‡t!")
            break
        if not question:
            continue

        # âŒ CÅ©: answer = ask_phogpt(question)
        # âœ… Má»›i: gá»i API LM Studio Ä‘á»ƒ láº¥y cÃ¢u tráº£ lá»i
        answer = ask_lmstudio(question)

        if answer:
            print("\nğŸ¤– LM Studio:", answer)
            print("-" * 60)
