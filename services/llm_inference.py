#*
# File n√†y ch·ª©a l√µi logic giao ti·∫øp tr·ª±c ti·∫øp v·ªõi model AI (PhoGPT) th√¥ng qua llama-cpp-python.
# N√≥ ch·ªãu tr√°ch nhi·ªám cho vi·ªác t·∫£i model, c·∫•u h√¨nh tham s·ªë, v√† ch·∫°y suy lu·∫≠n (inference).*#

import os
import requests  # üÜï Th√™m th∆∞ vi·ªán ƒë·ªÉ g·ªçi API LM Studio
# from rag_numpy_phogpt import RAGEngine  # ‚ùå COMMENT: D√≤ng n√†y d√πng llama-cpp, t·∫°m th·ªùi v√¥ hi·ªáu khi chuy·ªÉn sang LM Studio

# ===============================
# ‚öôÔ∏è C·∫§U H√åNH ƒê∆Ø·ªúNG D·∫™N
# ===============================
#MODEL_PATH = r"PhoGPT-4B-Chat-Q4_K_M.gguf"  # ‚ùå D√≤ng n√†y ch·ªâ d√πng khi ch·∫°y model qua llama-cpp
#DOC_FILE = r"D:\abode\pythonProject\ChatBot_Demo\data\luat_ban_hanh_vbqppl.txt"
INDEX_FOLDER = r"D:\abode\pythonProject\ChatBot_Demo\index"

# ===============================
# üöÄ KH·ªûI T·∫†O RAG + PHOGPT
# ===============================
# ‚ùå PHI√äN B·∫¢N C≈® ‚Äî ch·∫°y model n·ªôi b·ªô b·∫±ng llama-cpp
# rag_engine = RAGEngine(model_path=MODEL_PATH, n_threads=6)

# ‚úÖ PHI√äN B·∫¢N M·ªöI ‚Äî d√πng LM Studio API ƒë·ªÉ thay th·∫ø llama-cpp
LMSTUDIO_URL = "http://127.0.0.1:1234/v1/chat/completions"  # üÜï API c·ªßa LM Studio
HEADERS = {"Content-Type": "application/json"}  # üÜï Header cho request
# MODEL_NAME = "lmstudio-community/Meta-Llama-3-8B-Instruct"  # ‚ö†Ô∏è S·ª≠a t√™n theo model ƒëang ch·∫°y trong LM Studio
MODEL_NAME = "phogpt-4b-chat"  # ‚ö†Ô∏è S·ª≠a t√™n theo model ƒëang ch·∫°y trong LM Studio

# ===============================
# üìö KI·ªÇM TRA HO·∫∂C X√ÇY D·ª∞NG CH·ªà M·ª§C
# ===============================
# ‚ùó RAGEngine d√πng llama-cpp n√™n ph·∫ßn n√†y ch·ªâ gi·ªØ logic ki·ªÉm tra file
# if not os.path.exists(INDEX_FOLDER) or len(os.listdir(INDEX_FOLDER)) == 0:
#     print("‚öôÔ∏è Kh√¥ng t√¨m th·∫•y ch·ªâ m·ª•c. (T√≠nh nƒÉng RAG t·∫°m th·ªùi b·ªã v√¥ hi·ªáu khi d√πng LM Studio)")
#     # ‚ùå C≈©: rag_engine.build_index_from_file(DOC_FILE, save_dir=INDEX_FOLDER)
# else:
#     print("‚úÖ ƒê√£ t√¨m th·∫•y ch·ªâ m·ª•c. (Ch∆∞a t√≠ch h·ª£p RAGEngine v·ªõi LM Studio)")
#     # ‚ùå C≈©: rag_engine.load_index(INDEX_FOLDER)

# ===============================
# üí¨ H√ÄM H·ªéI CHATBOT ‚Äî PHI√äN B·∫¢N M·ªöI
# ===============================
# def ask_lmstudio(prompt):
#     print(f"\nüß† C√¢u h·ªèi: {prompt}\n")
#     try:
#         # üÜï G·ª≠i prompt t·ªõi LM Studio API (thay v√¨ ch·∫°y model n·ªôi b·ªô)
#         data = {
#             "model": MODEL_NAME,
#             "messages": [
#                 {"role": "system", "content": "B·∫°n l√† chatbot h·ªó tr·ª£ c√¥ng d√¢n Vi·ªát Nam, tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† ch√≠nh x√°c."},
#                 {"role": "user", "content": prompt}
#             ]
#         }
#
#         response = requests.post(LMSTUDIO_URL, headers=HEADERS, json=data)
#         answer = response.json()["choices"][0]["message"]["content"]
#         print(f"\n‚úÖ K·∫øt lu·∫≠n: {answer}")
#         return answer
#     except Exception as e:
#         print(f"‚ùå L·ªói khi g·ªçi LM Studio API: {e}")
#         return None
from services.rag_engine import load_index,vector_search_boosted
# def ask_lmstudio(query):
#     print(f"\nüß† C√¢u h·ªèi: {query}\n")
#     vectorizer, tfidf_matrix, loaded_chunks = load_index("NghiDinhDatDai")
#
#     retrieved_chunks = vector_search_boosted(query, vectorizer, tfidf_matrix, loaded_chunks, k=4, boost_factor=5)
#
#     print("\n--- K·∫æT QU·∫¢ TR√çCH XU·∫§T C·∫¢I TI·∫æN (BOOSTED RETRIEVAL) ---")
#     for i, res in enumerate(retrieved_chunks, start=1):
#         print(f"\nTop {i} ‚Äî score boosted: {res['score_boosted']:.4f}, score original: {res['score_original']:.4f}")
#         print(res['content'])
#         print(res['metadata'])
#
#
#     context_texts = "\n".join([chunk['content'] for chunk in retrieved_chunks])
#     prompt = f"B·∫°n l√† tr·ª£ l√Ω AI, tr·∫£ l·ªùi d·ª±a tr√™n d·ªØ li·ªáu sau:\n{context_texts}\n\nC√¢u h·ªèi: {query}"
#     print(prompt)
#     try:
#         data = {
#             "model": MODEL_NAME,
#             "messages": [
#                 {"role": "system", "content": "B·∫°n l√† chatbot h·ªó tr·ª£ c√¥ng d√¢n Vi·ªát Nam, tr·∫£ l·ªùi ng·∫Øn g·ªçn v√† ch√≠nh x√°c."},
#                 {"role": "user", "content": prompt}
#             ]
#         }
#         response = requests.post(LMSTUDIO_URL, headers=HEADERS, json=data)
#         answer = response.json()["choices"][0]["message"]["content"]
#         print(f"\n‚úÖ K·∫øt lu·∫≠n: {answer}")
#         return answer
#     except Exception as e:
#         print(f"‚ùå L·ªói khi g·ªçi LM Studio API: {e}")
#         return None
def ask_lmstudio(query):
    print(f"\nüß† C√¢u h·ªèi: {query}\n")

    # 1. Load index ƒë√£ t·∫°o
    vectorizer, tfidf_matrix, loaded_chunks = load_index("NghiDinhThue")

    # 2. Tr√≠ch xu·∫•t top K chunk d·ª±a tr√™n t√¨m ki·∫øm cosine + boosting
    retrieved_chunks = vector_search_boosted(
        query,
        vectorizer,
        tfidf_matrix,
        loaded_chunks,
        k=4,            # s·ªë chunk top
        boost_factor=5  # h·ªá s·ªë tƒÉng c∆∞·ªùng
    )

    # 3. In k·∫øt qu·∫£ tr√≠ch xu·∫•t ƒë·ªÉ debug
    print("\n--- K·∫æT QU·∫¢ TR√çCH XU·∫§T C·∫¢I TI·∫æN (BOOSTED RETRIEVAL) ---")
    for i, res in enumerate(retrieved_chunks, start=1):
        meta = res['metadata']
        print(f"\nTop {i} ‚Äî score boosted: {res['score_boosted']:.4f}, score original: {res['score_original']:.4f}")
        print(f"N·ªôi dung: {res['content']}")
        print(f"Ngu·ªìn: {meta['Decree']}, {meta['Chapter']}, {meta['article_number']} - {meta['article']}, Kho·∫£n: {meta['Clause']}")

    # 4. T·∫°o prompt chi ti·∫øt, c√≥ metadata ƒë·ªÉ LM Studio tr·∫£ l·ªùi c√≥ d·∫´n ngu·ªìn
    context_texts = ""
    for res in retrieved_chunks:
        meta = res['metadata']
        context_texts += (
            f"[Ngu·ªìn: Ngh·ªã ƒë·ªãnh: {meta['Decree']}, Ch∆∞∆°ng: {meta['Chapter']}, "
            f"ƒêi·ªÅu {meta['article_number']} - {meta['article']}, Kho·∫£n: {meta['Clause']}]\n"
            f"{res['content']}\n\n"
        )

    prompt = (
        "B·∫°n l√† tr·ª£ l√Ω AI ti·∫øng Vi·ªát.\n"
        # "H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y.\n"
        "M·ªói th√¥ng tin ƒë∆∞·ª£c tr√≠ch d·∫´n ph·∫£i n√™u r√µ Ngh·ªã ƒë·ªãnh, Ch∆∞∆°ng, ƒêi·ªÅu, Kho·∫£n.\n\n"
        f"--- D·ªÆ LI·ªÜU ---\n{context_texts}\n"
        f"--- C√ÇU H·ªéI ---\n{query}\n\n"
        "=== TR·∫¢ L·ªúI C√ì D·∫™N NGU·ªíN ==="
    )

    print("\nüìú Prompt g·ª≠i t·ªõi LM Studio:")
    print(prompt)

    # 5. G·ªçi LM Studio API
    try:
        data = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": "B·∫°n l√† chatbot h·ªó tr·ª£ c√¥ng d√¢n Vi·ªát Nam, tr·∫£ l·ªùi ng·∫Øn g·ªçn, r√µ r√†ng v√† c√≥ d·∫´n ngu·ªìn."},
                {"role": "user", "content": prompt}
            ]
        }
        response = requests.post(LMSTUDIO_URL, headers=HEADERS, json=data)
        answer = response.json()["choices"][0]["message"]["content"]
        print(f"\n‚úÖ C√¢u tr·∫£ l·ªùi:\n{answer}")
        return answer
    except Exception as e:
        print(f"‚ùå L·ªói khi g·ªçi LM Studio API: {e}")
        return None

# ===============================
# üßë‚Äçüíª V√íNG L·∫∂P CHAT
# ===============================
if __name__ == "__main__":
    print("\nüí¨ ChatBot LM Studio s·∫µn s√†ng! H√£y ƒë·∫∑t c√¢u h·ªèi (g√µ 'exit' ho·∫∑c 'tho√°t' ƒë·ªÉ d·ª´ng).")
    while True:
        question = input("\nB·∫°n: ").strip()
        if question.lower() in ["exit", "quit", "q", "tho√°t"]:
            print("üëã T·∫°m bi·ªát!")
            break
        if not question:
            continue

        # ‚ùå C≈©: answer = ask_phogpt(question)
        # ‚úÖ M·ªõi: g·ªçi API LM Studio ƒë·ªÉ l·∫•y c√¢u tr·∫£ l·ªùi
        answer = ask_lmstudio(question)

        if answer:
            print("\nü§ñ LM Studio:", answer)
            print("-" * 60)
