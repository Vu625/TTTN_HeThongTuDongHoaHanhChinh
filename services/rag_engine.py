#Ch·ª©a c√°c h√†m Numpy ƒë·ªÉ Vector Search, Similarity Search (Custom RAG).
import os
import numpy as np
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import streamlit as st
from pathlib import Path
from collections import Counter
import math
from scipy.sparse import csr_matrix, save_npz, load_npz
from scipy.sparse.linalg import norm as sparse_norm
import glob
import pickle
BASE_DIR = Path(__file__).resolve().parent.parent   # nh·∫£y ra kh·ªèi services/
LAW_DIR = BASE_DIR / "data/db/law_texts"
Path_Vecto = Path("data/db/database_vecto")
def load_law_texts():
    texts = []
    for file in LAW_DIR.glob("*.txt"):
        with open(file, "r", encoding="utf-8") as f:
            texts.append((file.name, f.read()))
    return texts

def build_vector_store():
    texts = load_law_texts()
    docs = [t[1] for t in texts]
    vectorizer = TfidfVectorizer(stop_words="english")
    vectors = vectorizer.fit_transform(docs)
    return texts, vectorizer, vectors

def retrieve_relevant_text(query, vectorizer, vectors, texts, top_k=1):
    query_vec = vectorizer.transform([query])
    similarities = cosine_similarity(query_vec, vectors).flatten()
    top_indices = similarities.argsort()[-top_k:][::-1]
    results = [(texts[i][0], texts[i][1], similarities[i]) for i in top_indices]
    return results

def generate_answer(query):
    texts, vectorizer, vectors = build_vector_store()
    results = retrieve_relevant_text(query, vectorizer, vectors, texts)
    if not results:
        return "Xin l·ªói, t√¥i kh√¥ng t√¨m th·∫•y quy ƒë·ªãnh ph√π h·ª£p."
    filename, content, score = results[0]
    return f"üìò Theo **{filename}**:\n\n{content.strip()}\n\n(ƒê·ªô li√™n quan: {score:.2f})"

def read_txt(filename):
    file_path = (LAW_DIR / filename).resolve()
    print("üîç ƒêang load file:", file_path)
    if not file_path.exists():
        print(f"‚ö†Ô∏è File {file_path} kh√¥ng t·ªìn t·∫°i!")
        return ""
    with open(file_path, 'r', encoding='utf-8') as f:
        return f.read()
def split_into_chunks(text):
    chunks = []

    # 1) T√°ch t√™n Ngh·ªã ƒë·ªãnh (gi·∫£ s·ª≠ d√≤ng ƒë·∫ßu ho·∫∑c c√≥ "Ngh·ªã ƒë·ªãnh")
    match_nghidinh = re.search(r'(Ngh·ªã ƒë·ªãnh.*?)(\n|$)', text, re.IGNORECASE)
    nghidinh_title = ""
    if match_nghidinh:
        nghidinh_title = match_nghidinh.group(1).strip()
        # Lo·∫°i b·ªè ph·∫ßn ti√™u ƒë·ªÅ ra kh·ªèi text
        text = text[match_nghidinh.end():].strip()

    # 2) T√°ch theo Ch∆∞∆°ng
    chuong_pattern = r'(Ch∆∞∆°ng\s+\w+.*?)(?=\nCh∆∞∆°ng|\Z)'
    chuong_list = re.findall(chuong_pattern, text, flags=re.DOTALL)

    for chuong_block in chuong_list:
        # L·∫•y t√™n Ch∆∞∆°ng (d√≤ng ƒë·∫ßu)
        chuong_title = chuong_block.split("\n")[0].strip()

        # N·ªôi dung c√≤n l·∫°i c·ªßa Ch∆∞∆°ng
        chuong_content = chuong_block[len(chuong_title):].strip()

        # 3) T√°ch theo ƒêi·ªÅu
        dieu_pattern = r'(ƒêi·ªÅu\s+\d+\.)'
        dieu_splits = re.split(dieu_pattern, chuong_content)

        for i in range(1, len(dieu_splits), 2):
            dieu_number = dieu_splits[i].strip()
            dieu_title = re.split('\n', dieu_splits[i+1])[0]
            dieu_content = dieu_splits[i+1]

            # 4) T√°ch theo Kho·∫£n
            khoan_pattern = r'(?m)^(\d+\.)'
            khoan_splits = re.split(khoan_pattern, dieu_content)

            if len(khoan_splits) == 1:
                # N·∫øu ƒëi·ªÅu kh√¥ng c√≥ kho·∫£n ‚Üí chunk c·∫£ ƒëi·ªÅu
                # chunk = (
                #     f"{nghidinh_title}\n"
                #     f"{chuong_title}\n"
                #     f"{dieu_title}\n"
                #     f"{khoan_splits[0].strip()}"
                # )
                # chunks.append(chunk)
                chunks.append({
                    'content': khoan_splits[0].strip(),
                    'metadata': {
                        'Decree': nghidinh_title,
                        'Chapter': chuong_title,
                        'article_number': dieu_number,
                        'article': dieu_title,
                        'Clause': " ",
                    }
                })
            else:
                # N·∫øu c√≥ kho·∫£n ‚Üí m·ªói kho·∫£n 1 chunk
                for k in range(1, len(khoan_splits), 2):
                    khoan_num = khoan_splits[k].replace(".", "")
                    khoan_text = khoan_splits[k+1].strip()

                    # chunk = (
                    #     f"{nghidinh_title}\n"
                    #     f"{chuong_title}\n"
                    #     f"{dieu_title} Kho·∫£n {khoan_num}\n"
                    #     f"{khoan_text}"
                    # )
                    # chunks.append(chunk)
                    chunks.append({
                        'content': khoan_text,
                        'metadata': {
                            'Decree': nghidinh_title,
                            'Chapter': chuong_title,
                            'article_number': dieu_number,
                            'article': dieu_title,
                            'Clause': khoan_num,
                        }
                    })

    return chunks

class CustomTfidfVectorizer:
    def __init__(self, stop_words=None):
        self.vocabulary = {}
        self.idf = {}
        self.stop_words = set(stop_words) if stop_words else set()

    def _tokenize(self, text):
        return re.findall(r'\b\w+\b', text.lower())

    def fit(self, raw_documents):
        term_document_frequency = Counter()
        document_count = len(raw_documents)

        for doc in raw_documents:
            tokens = self._tokenize(doc)
            unique_tokens = set(tokens)
            term_document_frequency.update(unique_tokens)

        # self.vocabulary = {term: idx for idx, term in enumerate(term_document_frequency.keys())
        #                    if term not in self.stop_words}
        valid_terms = [term for term in term_document_frequency.keys()
                       if term not in self.stop_words]

        # Ch·ªâ s·ªë ph·∫£i ƒë∆∞·ª£c g√°n tu·∫ßn t·ª± d·ª±a tr√™n danh s√°ch ƒë√£ l·ªçc
        self.vocabulary = {term: idx for idx, term in enumerate(valid_terms)}

        self.idf = {
            term: math.log((document_count + 1) / (df + 1)) + 1
            for term, df in term_document_frequency.items()
            if term in self.vocabulary
        }
        return self

    def transform(self, raw_documents):
        num_docs = len(raw_documents)
        num_features = len(self.vocabulary)
        data, row_ind, col_ind = [], [], []

        for doc_index, doc in enumerate(raw_documents):
            tokens = self._tokenize(doc)
            term_counts = Counter(tokens)
            total_tokens = len(tokens)

            for term, count in term_counts.items():
                if term in self.vocabulary:
                    term_index = self.vocabulary[term]
                    tf = count / total_tokens
                    idf = self.idf.get(term, 0)
                    tfidf_score = tf * idf
                    data.append(tfidf_score)
                    row_ind.append(doc_index)
                    col_ind.append(term_index)

        tfidf_matrix = csr_matrix((data, (row_ind, col_ind)), shape=(num_docs, num_features))
        norms = np.sqrt(tfidf_matrix.power(2).sum(axis=1))
        norms[norms == 0] = 1
        tfidf_matrix = tfidf_matrix.multiply(1 / norms)
        return tfidf_matrix

# ===============================
# Save/Load Index
# ===============================
# def save_index(vectorizer, tfidf_matrix, chunks, index_prefix):
#     with open(Path_Vecto / f"{index_prefix}_vectorizer.pkl", "wb") as f:
#         pickle.dump(vectorizer, f)
#     save_npz(Path_Vecto / f"{index_prefix}_matrix.npz", tfidf_matrix)
#     with open(Path_Vecto / f"{index_prefix}_chunks.pkl", 'wb') as f:
#         pickle.dump(chunks, f)
#     print("‚úÖ ƒê√£ l∆∞u tr·ªØ Index th√†nh c√¥ng!")

from pathlib import Path
import pickle
from scipy.sparse import save_npz

# BASE_DIR: th∆∞ m·ª•c g·ªëc c·ªßa project
BASE_DIR = Path(__file__).resolve().parent.parent

# VECTOR_DIR: th∆∞ m·ª•c l∆∞u vector
VECTOR_DIR = BASE_DIR / "data/db/database_vecto"


def save_index(vectorizer, tfidf_matrix, chunks, index_prefix):

    # üî• T·∫°o folder n·∫øu ch∆∞a c√≥
    VECTOR_DIR.mkdir(parents=True, exist_ok=True)

    # üî• In ra ƒë·ªÉ debug
    print("üìÇ L∆∞u index v√†o:", VECTOR_DIR.resolve())

    # --- L∆∞u vectorizer ---
    vec_path = VECTOR_DIR / f"{index_prefix}_vectorizer.pkl"
    with open(vec_path, "wb") as f:
        pickle.dump(vectorizer, f)

    # --- L∆∞u matrix ---
    mat_path = VECTOR_DIR / f"{index_prefix}_tfidf_matrix.npz"
    save_npz(mat_path, tfidf_matrix)

    # --- L∆∞u chunks ---
    chunks_path = VECTOR_DIR / f"{index_prefix}_chunks.pkl"
    with open(chunks_path, "wb") as f:
        pickle.dump(chunks, f)

    print("‚úÖ L∆∞u th√†nh c√¥ng:", index_prefix)

# def load_index(index_prefix):
#     with open(Path_Vecto / f"{index_prefix}_vectorizer.pkl", 'rb') as f:
#         vectorizer = pickle.load(f)
#     tfidf_matrix = load_npz(Path_Vecto / f"{index_prefix}_matrix.npz")
#     with open(Path_Vecto / f"{index_prefix}_chunks.pkl", 'rb') as f:
#         chunks = pickle.load(f)
#     return vectorizer, tfidf_matrix, chunks

def load_index(index_prefix):

    # üîç Debug: xem ƒë∆∞·ªùng d·∫´n th·ª±c t·∫ø
    print("üìÇ ƒêang load index t·ª´:", VECTOR_DIR.resolve())

    vec_path = VECTOR_DIR / f"{index_prefix}_vectorizer.pkl"
    mat_path = VECTOR_DIR / f"{index_prefix}_tfidf_matrix.npz"
    chunks_path = VECTOR_DIR / f"{index_prefix}_chunks.pkl"

    # --- Ki·ªÉm tra file t·ªìn t·∫°i ---
    if not vec_path.exists():
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y: {vec_path}")
    if not mat_path.exists():
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y: {mat_path}")
    if not chunks_path.exists():
        raise FileNotFoundError(f"‚ùå Kh√¥ng t√¨m th·∫•y: {chunks_path}")

    # --- Load vectorizer ---
    with open(vec_path, "rb") as f:
        vectorizer = pickle.load(f)

    # --- Load matrix ---
    tfidf_matrix = load_npz(mat_path)

    # --- Load chunks ---
    with open(chunks_path, "rb") as f:
        chunks = pickle.load(f)

    print(f"‚úÖ Load th√†nh c√¥ng: {index_prefix}")
    return vectorizer, tfidf_matrix, chunks
def vector_search_boosted(query, vectorizer, tfidf_matrix, chunks, k=5, boost_factor=1.5):
    query_vector = vectorizer.transform([query])
    if query_vector.shape[1] == 0:
        return []
    q_norm = sparse_norm(query_vector)
    if q_norm == 0:
        return []
    normalized_query = query_vector.multiply(1 / q_norm)
    similarity_scores = normalized_query.dot(tfidf_matrix.T).toarray()[0]

    primary_keywords = ["nguy√™n t·∫Øc", "ph·∫°m vi", "h·ªá th·ªëng", "th·∫©m quy·ªÅn", "hi·ªáu l·ª±c", "gi·∫£i th√≠ch"]
    boosted_scores = np.copy(similarity_scores)
    query_lower = query.lower()
    is_query_focused_on_primary_keyword = any(kw in query_lower for kw in primary_keywords)

    if is_query_focused_on_primary_keyword:
        for idx, chunk in enumerate(chunks):
            first_line_content = chunk['content'].split('\n')[0].lower()
            should_boost = any(kw in first_line_content and kw in query_lower for kw in primary_keywords)
            if should_boost:
                boosted_scores[idx] = similarity_scores[idx] * boost_factor

    top_indices = np.argsort(boosted_scores)[::-1][:k]
    results = []
    for idx in top_indices:
        if boosted_scores[idx] > 0:
            results.append({
                'score_boosted': boosted_scores[idx],
                'score_original': similarity_scores[idx],
                'content': chunks[idx]['content'],
                'metadata': chunks[idx]['metadata']
            })
    return results

def prepare(file_name):
    word_stop = ["l√†","th√¨","c·ªßa"]
    text = read_txt(file_name)
    chunks = split_into_chunks(text)
    raw_texts = [chunk['content'] for chunk in chunks]
    vectorizer = CustomTfidfVectorizer(stop_words=word_stop).fit(raw_texts)
    tfidf_matrix = vectorizer.transform(raw_texts)
    print(f"‚úÖ Ma tr·∫≠n TF-IDF ƒë√£ t·∫°o: **{tfidf_matrix.shape}**")
    name = file_name.split('.')[0]
    save_index(vectorizer, tfidf_matrix, chunks, name)

# prepare("NghiDinhThue.txt")


def bulk_prepare_and_index(directory_path, index_prefix="law_engine_full"):
    all_chunks = []
    stop_words = ["l√†", "th√¨", "c·ªßa"]
    directory_path = BASE_DIR / directory_path
    # 1. L·∫∑p qua t·∫•t c·∫£ c√°c file .txt trong th∆∞ m·ª•c
    search_pattern = os.path.join(directory_path, "*.txt")
    file_paths = glob.glob(search_pattern)
    if not file_paths:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file .txt n√†o trong th∆∞ m·ª•c: {directory_path}")
        return

    print(f"‚úÖ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(file_paths)} file lu·∫≠t...")

    for file_path in file_paths:
        file_name = os.path.basename(file_path)
        try:
            # S·ª≠ d·ª•ng h√†m chunking ƒë√£ c√≥
            text = read_txt(file_path)
            chunks = split_into_chunks(text)
            # chunks = load_and_chunk_law_data(file_path)

            # C·∫≠p nh·∫≠t metadata: Th√™m t√™n file g·ªëc ƒë·ªÉ truy v·∫øt
            # ƒêi·ªÅu n√†y r·∫•t quan tr·ªçng ƒë·ªÉ bi·∫øt chunk ƒë√≥ ƒë·∫øn t·ª´ Lu·∫≠t n√†o
            for chunk in chunks:
                chunk['metadata']['source_file'] = file_name

            all_chunks.extend(chunks)
            print(f"   -> ƒê√£ chunk {len(chunks)} ƒëo·∫°n t·ª´ file: {file_name}")

        except Exception as e:
            print(f"   -> ‚ö†Ô∏è L·ªói khi x·ª≠ l√Ω file {file_name}: {e}")

    print(f"T·ªïng s·ªë chunks ƒë√£ thu th·∫≠p: {len(all_chunks)}")

    if not all_chunks:
        return

    # 2. Vector H√≥a To√†n b·ªô T·∫≠p D·ªØ li·ªáu (D√≤ng n√†y gom t·∫•t c·∫£ ki·∫øn th·ª©c)
    raw_texts = [chunk['content'] for chunk in all_chunks]

    vectorizer = CustomTfidfVectorizer(stop_words=set(stop_words) if stop_words else None).fit(raw_texts)
    tfidf_matrix = vectorizer.transform(raw_texts)

    print(f"‚úÖ Ma tr·∫≠n TF-IDF ƒë√£ t·∫°o v·ªõi k√≠ch th∆∞·ªõc: {tfidf_matrix.shape}")

    # 3. L∆∞u tr·ªØ Index
    save_index(vectorizer, tfidf_matrix, all_chunks, index_prefix=index_prefix)

    print(f"üéâ Ho√†n t·∫•t Indexing. ƒê√£ l∆∞u 3 file index v·ªõi prefix: {index_prefix}")
# word_stop = ["l√†","th√¨","c·ªßa"]
# bulk_prepare_and_index(BASE_DIR / "data/db/law_texts", stop_words=word_stop)